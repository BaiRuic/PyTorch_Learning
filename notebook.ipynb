{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax\n",
    "+ 使得最后每一行之和为1 ，即将之前无意义的输出转换为一个概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum(dim=1, keepdim=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2501, 0.3376, 0.4123],\n",
       "        [0.2603, 0.3514, 0.3883]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.tensor([[0.5,0.8,1],\n",
    "                      [1.2,1.5,1.6]])\n",
    "x_softmax = softmax(test_x)\n",
    "x_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3060, 0.3340, 0.3599],\n",
       "        [0.3094, 0.3389, 0.3517]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3243, 0.3335, 0.3422],\n",
       "        [0.3254, 0.3351, 0.3394]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注，重复多次的取softmax 依然会变化\n",
    "softmax(softmax(x_softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "+ 注意交叉熵最后取的是所有样本交叉熵的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 法1\n",
    "def cross_loss_1(y_hat,y):\n",
    "    # loss 列表 用来存放 每一个样本的损失值\n",
    "    loss=[]\n",
    "    # len(y) 个样本\n",
    "    for i in range(len(y)):\n",
    "        loss.append ( - torch.log(y_hat[i][y[i]]).item() )    # 注意取 item() 和 不取item() 的区别\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 法2\n",
    "def cross_loss_2(y_hat, y):\n",
    "    return -torch.log(y_hat.gather(1,y.view(-1,1)))  # gather 收集输入的特定维度指定位置的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2501, 0.3376, 0.4123],\n",
       "        [0.2603, 0.3514, 0.3883]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.tensor([[0.5,0.8,1],\n",
    "                      [1.2,1.5,1.6]])\n",
    "y = torch.tensor([2,1])\n",
    "y_hat = softmax(y_hat)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对全部概率取-log：\n",
      "tensor([[-1.3859, -1.0859, -0.8859],\n",
      "        [-1.3459, -1.0459, -0.9459]])\n",
      "\n",
      "\n",
      "对真实类别对应概率取-log：\n",
      "法1 [0.8859393000602722, 1.0459107160568237]\n",
      "法2 tensor([[0.8859],\n",
      "        [1.0459]])\n"
     ]
    }
   ],
   "source": [
    "# 观察取 交叉损失的过程 \n",
    "# 交叉损失 就是对  样本真实类别的 对应的概率值 取 log\n",
    "# 在y_hat 第一行取了第2个元素\n",
    "# 在 y_hat第二行取了第1个元素\n",
    "# 2和1 分别为样本的真实值 ,y=[2，1]\n",
    "print('对全部概率取-log：')\n",
    "print(torch.log(y_hat))\n",
    "print('\\n')\n",
    "print('对真实类别对应概率取-log：')\n",
    "print('法1',cross_loss_1(y_hat,y))\n",
    "print('法2',cross_loss_2(y_hat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_hat: tensor([[1.5000, 1.9000, 2.1000],\n",
      "        [5.6000, 6.1000, 4.9000],\n",
      "        [3.5000, 3.9000, 1.2000],\n",
      "        [9.8000, 9.0000, 7.8000]]) \n",
      "\n",
      "y_hat: tensor([[0.2318, 0.3458, 0.4224],\n",
      "        [0.3179, 0.5242, 0.1579],\n",
      "        [0.3858, 0.5755, 0.0387],\n",
      "        [0.6310, 0.2835, 0.0854]]) \n",
      "\n",
      "y: [1, 0, 1, 2] \n",
      "\n",
      "log_all: tensor([[1.4619, 1.0619, 0.8619],\n",
      "        [1.1459, 0.6459, 1.8459],\n",
      "        [0.9525, 0.5525, 3.2525],\n",
      "        [0.4604, 1.2604, 2.4604]]) \n",
      "\n",
      "loss: [1.0618523359298706, 1.1459113359451294, 0.5524619817733765, 2.4603724479675293] \n",
      "\n",
      "loss_mean tensor(1.3051)\n",
      "crossentropyloss_output: tensor(1.3051)\n"
     ]
    }
   ],
   "source": [
    "# 测试 \n",
    "# 假设4个样本 3个训练出来的 结果为\n",
    "o_hat = torch.tensor([[1.5, 1.9, 2.1],\n",
    "                     [5.6,6.1,4.9],\n",
    "                     [3.5,3.9,1.2],\n",
    "                     [9.8,9.0,7.8]])\n",
    "print('o_hat:',o_hat,'\\n')\n",
    "# softmax \n",
    "y_hat = softmax(o_hat)\n",
    "print('y_hat:',y_hat,'\\n')\n",
    "\n",
    "#样本真实值\n",
    "y = [1,0,1,2]\n",
    "y_target = torch.tensor(y)\n",
    "print('y:',y,'\\n')\n",
    "\n",
    "# 对所有概率值 取负对数\n",
    "print('log_all:',-torch.log(y_hat),'\\n')\n",
    "\n",
    "# 交叉损失值\n",
    "loss = cross_loss_1(y_hat,y)\n",
    "print('loss:',loss,'\\n')\n",
    "# 对交叉熵取均值\n",
    "print('loss_mean',torch.tensor(loss).mean())\n",
    "\n",
    "# 直接用 torch.nn.CrossEntropyLoss函数\n",
    "crossentropyloss=torch.nn.CrossEntropyLoss()\n",
    "crossentropyloss_output=crossentropyloss(o_hat,torch.tensor(y))\n",
    "print('crossentropyloss_output:',crossentropyloss_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion_MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据集\n",
    "# torchvision.datasets 里面有很多数据集合 \n",
    "def load_data_fashion_MNIST(batch_size):\n",
    "    # torchvision.datasets 里面有很多数据集合 ,从这里加载 到本地临时文件夹\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMnist', train=True, download=True,transform=torchvision.transforms.ToTensor()) # train=True 是训练集\n",
    "    mnist_test =  torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMnist',train=False, download=True,transform=torchvision.transforms.ToTensor()) #train=False 是测试集\n",
    "    # 将加载的数据集转化为可迭代批量数据集\n",
    "    # 用到 torch.utils.data 里面的函数\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=0)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=True, num_workers=0)\n",
    "    return train_iter,test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## 观察数据集合\n",
    "batch_size=256\n",
    "train_iter,test_iter = load_data_fashion_MNIST(batch_size)\n",
    "\n",
    "for X,y in train_iter:\n",
    "    print(X.shape,y.shape)\n",
    "    break\n",
    "# 可以看到 每次迭代出来的一批数据大小为batch_size\n",
    "# for 循环一直执行下去，会遍历完所有的样本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
